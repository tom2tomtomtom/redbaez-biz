{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECD-Eye POC: Model Evaluation\n",
    "\n",
    "This notebook analyzes the results of the blind evaluation to determine if the fine-tuned model performs better than the baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('ggplot')\n",
    "sns.set_palette(\"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Evaluation Results\n",
    "\n",
    "First, let's load the results of the blind evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Paths\n",
    "DATA_DIR = Path(\"../data\")\n",
    "RESULTS_FILE = DATA_DIR / \"evaluation_results.csv\"\n",
    "\n",
    "# Load results\n",
    "results_df = pd.read_csv(RESULTS_FILE)\n",
    "print(f\"Loaded {len(results_df)} evaluation results\")\n",
    "\n",
    "# Display results\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Analyze Results\n",
    "\n",
    "Let's analyze the results to see which model performed better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Count preferences\n",
    "model_counts = results_df[\"preferred_model\"].value_counts()\n",
    "print(\"Model Preferences:\")\n",
    "print(model_counts)\n",
    "\n",
    "# Calculate percentages\n",
    "model_percentages = model_counts / len(results_df) * 100\n",
    "print(\"\\nModel Preferences (%)\")\n",
    "print(model_percentages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize preferences\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.barplot(x=model_counts.index, y=model_counts.values)\n",
    "plt.title(\"Model Preferences\")\n",
    "plt.xlabel(\"Model\")\n",
    "plt.ylabel(\"Count\")\n",
    "\n",
    "# Add percentage labels\n",
    "for i, v in enumerate(model_counts.values):\n",
    "    ax.text(i, v + 0.1, f\"{model_percentages[i]:.1f}%\", ha=\"center\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Statistical Analysis\n",
    "\n",
    "Let's perform a binomial test to determine if the fine-tuned model is significantly better than the baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Count successes (fine-tuned model preferred)\n",
    "n_finetuned = model_counts.get(\"finetuned\", 0)\n",
    "n_total = len(results_df)\n",
    "\n",
    "# Perform binomial test\n",
    "p_value = stats.binom_test(n_finetuned, n_total, p=0.5, alternative=\"greater\")\n",
    "\n",
    "print(f\"Fine-tuned model preferred: {n_finetuned}/{n_total} ({n_finetuned/n_total*100:.1f}%)\")\n",
    "print(f\"p-value: {p_value:.4f}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(\"The fine-tuned model is significantly better than the baseline model.\")\n",
    "else:\n",
    "    print(\"The difference between models is not statistically significant.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyze Taglines\n",
    "\n",
    "Let's analyze the taglines to understand what characteristics the ECD prefers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Add word count column\n",
    "results_df[\"baseline_word_count\"] = results_df[\"baseline_tagline\"].str.split().str.len()\n",
    "results_df[\"finetuned_word_count\"] = results_df[\"finetuned_tagline\"].str.split().str.len()\n",
    "results_df[\"preferred_word_count\"] = results_df[\"preferred_tagline\"].str.split().str.len()\n",
    "\n",
    "# Display word counts\n",
    "print(\"Word Count Statistics:\")\n",
    "print(f\"Baseline: {results_df['baseline_word_count'].mean():.1f} words on average\")\n",
    "print(f\"Fine-tuned: {results_df['finetuned_word_count'].mean():.1f} words on average\")\n",
    "print(f\"Preferred: {results_df['preferred_word_count'].mean():.1f} words on average\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize word counts\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(data=results_df[[\"baseline_word_count\", \"finetuned_word_count\", \"preferred_word_count\"]])\n",
    "plt.title(\"Word Count Distribution\")\n",
    "plt.xlabel(\"Model\")\n",
    "plt.ylabel(\"Word Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compare Taglines\n",
    "\n",
    "Let's compare the taglines from both models side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create a comparison table\n",
    "comparison_df = results_df[[\"brief\", \"baseline_tagline\", \"finetuned_tagline\", \"preferred_model\"]]\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate Demo Slides Content\n",
    "\n",
    "Let's generate content for the demo slides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate slide content\n",
    "print(\"# ECD-Eye POC Results\\n\")\n",
    "print(f\"## Fine-tuned model preferred: {n_finetuned}/{n_total} ({n_finetuned/n_total*100:.1f}%)\\n\")\n",
    "print(f\"p-value: {p_value:.4f}\\n\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(\"**The fine-tuned model is significantly better than the baseline model.**\\n\")\n",
    "else:\n",
    "    print(\"**The difference between models is not statistically significant.**\\n\")\n",
    "\n",
    "print(\"## Example Taglines\\n\")\n",
    "\n",
    "for _, row in results_df.iterrows():\n",
    "    print(f\"### Brief: {row['brief']}\\n\")\n",
    "    print(f\"* Baseline: \\\"{row['baseline_tagline']}\\\"\")\n",
    "    print(f\"* Fine-tuned: \\\"{row['finetuned_tagline']}\\\"\")\n",
    "    print(f\"* **Preferred: {row['preferred_model'].capitalize()}**\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusion\n",
    "\n",
    "Based on the analysis above, we can draw the following conclusions:\n",
    "\n",
    "1. The fine-tuned model was preferred in X out of Y cases (Z%).\n",
    "2. The p-value of the binomial test is P, which [is/is not] statistically significant.\n",
    "3. The fine-tuned model tends to generate taglines with [more/fewer] words on average.\n",
    "4. [Add any other observations about the taglines here.]\n",
    "\n",
    "Overall, this POC [demonstrates/does not demonstrate] that fine-tuning ChatGPT on ECD-ranked examples can improve the quality of generated taglines compared to using static prompts."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
